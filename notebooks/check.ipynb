{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944ace7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10277896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the VADER lexicon \n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e0b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b11cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_boa = pd.read_csv('../data/boa_cleaned.csv', parse_dates=['date'])\n",
    "df_cbe = pd.read_csv('../data/cbe_cleaned.csv', parse_dates=['date'])\n",
    "df_dashen = pd.read_csv('../data/dashen_cleaned.csv', parse_dates=['date'])\n",
    "df = pd.concat([df_boa, df_cbe, df_dashen])\n",
    "df.reset_index(inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8ae62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean each review\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove emojis\n",
    "    text = emoji.replace_emoji(text, replace='')  # removes all emojis\n",
    "    \n",
    "    # Remove non-ASCII characters (Amharic, etc.)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    \n",
    "    # Remove non-alphanumeric characters (except basic punctuation)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-]', ' ', text)\n",
    "\n",
    "    # Keep only letters and whitespace\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = text.lower()\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73f874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Compute sentiment for a given text.\n",
    "    Returns:\n",
    "      sentiment_label: 'positive', 'neutral', or 'negative'\n",
    "      sentiment_score: compound score as a float\n",
    "    \"\"\"\n",
    "    scores = sid.polarity_scores(text)\n",
    "    compound = scores['compound']\n",
    "    # Define thresholds for sentiment labels\n",
    "    if compound >= 0.05:\n",
    "        label = 'positive'\n",
    "    elif compound <= -0.05:\n",
    "        label = 'negative'\n",
    "    else:\n",
    "        label = 'neutral'\n",
    "    return label, compound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_cleaned'] = df['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a0484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bank'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76003dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['sentiment_label', 'sentiment_score']] = df['review'].apply(lambda x: pd.Series(get_sentiment(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ddbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df,x='sentiment_label', hue='bank')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment labels for Dashen, CBE and Abyssinia banks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5160785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing includes lemmatization and stopword removal, \n",
    "which improves keyword extraction quality.\n",
    "\"\"\"\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text:\n",
    "    - Lowercase\n",
    "    - Remove stopwords\n",
    "    - Lemmatize\n",
    "    - Keep only alphabetic tokens\n",
    "    \"\"\"\n",
    "    # Check if the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Use spaCy to process the text\n",
    "    doc = nlp(text.lower())\n",
    "    \n",
    "    # Token filtering: remove stopwords and non-alphabetic tokens, apply lemmatization\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
    "    \n",
    "    # Join tokens back to a string\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb044f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with cleaned text\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e056e941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?\\'\"-]', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e57473",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_cleaned'] =  df['review'].apply(apply_text_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab66177",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['review', 'cleaned_review','sentiment_label']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(data=df,x='sentiment_label', hue='bank')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment labels for Dashen, CBE and Abyssinia banks')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfdfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Load your cleaned CSV which contains the critical 'review' column\n",
    "df = pd.read_csv('../data/cleaned_data.csv')\n",
    "\n",
    "# Step 1: Use TF-IDF Vectorizer to extract important keywords from each review\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = tfidf.fit_transform(df['review'])\n",
    "\n",
    "# Step 2: Cluster the reviews into themes using KMeans\n",
    "num_clusters = 3\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Assign a theme label to each review from the clustering results\n",
    "df['identified_theme'] = kmeans.labels_\n",
    "\n",
    "# View top keywords for each cluster\n",
    "terms = tfidf.get_feature_names_out()\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(num_clusters):\n",
    "    top_terms = [terms[ind] for ind in order_centroids[i, :10]]\n",
    "    print(f\"Cluster {i} top keywords: {top_terms}\")\n",
    "\n",
    "# Step 3: Export the DataFrame with the theme labels to a new CSV file\n",
    "df.to_csv('../data/cleaned_themed_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa313749",
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_counts = df.groupby(['bank', 'identified_theme']).size().reset_index(name='counts')\n",
    "\n",
    "# Visualize with a bar plot\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=theme_counts, x='identified_theme', y='counts', hue='bank')\n",
    "plt.title(\"Theme Distribution per Bank\")\n",
    "plt.xlabel(\"Identified Theme\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
